{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all required modules\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms, utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. About dataset\n",
    "Cifar-10 dataset is a dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images. There are 50000 training images and 10000 test images. Each image is 32x32 with 3 color channels (red, green, blue). The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# extra transfrom for the training data, in order to achieve better performance\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.RandomCrop(32, padding=2, padding_mode='reflect'), \n",
    "    transforms.RandomHorizontalFlip(), \n",
    "])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ResNet, train_model\n",
    "from models import device\n",
    "\n",
    "base_model = ResNet().to(device=device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(base_model.parameters(), betas = (0.851436, 0.999689), amsgrad=True, lr = 8e-5)\n",
    "loaders = {\"train\": trainloader, \"valid\": testloader}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight regularization pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weight_regularization import l1_s_norm\n",
    "\n",
    "_, t_1 = train_model(base_model, optimizer, criterion,\n",
    "                     loaders, N_epochs = 10, reg = l1_s_norm, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "amounts = 1e-2 * np.array([95, 97, 99, 99.5, 99.9])\n",
    "\n",
    "train_time = []\n",
    "acc = []\n",
    "\n",
    "for amount in amounts:\n",
    "\n",
    "    model = copy.deepcopy(base_model)\n",
    "    model = copy.deepcopy(base_model)\n",
    "\n",
    "    optimizer = optimizer = torch.optim.Adam(model.parameters(), betas = (0.851436, 0.999689), amsgrad=True, lr = 8e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    parameters = (\n",
    "    (model.conv1[0], 'weight'),\n",
    "    (model.conv2[0], 'weight'),\n",
    "    (model.res1[0][0], 'weight'),\n",
    "    (model.res1[1][0], 'weight'),\n",
    "    (model.conv3[0], 'weight'),\n",
    "    (model.conv4[0], 'weight'),\n",
    "    (model.res2[0][0], 'weight'),\n",
    "    (model.res2[1][0], 'weight'),\n",
    "    (model.classifier[2], 'weight')\n",
    "              )\n",
    "\n",
    "    prune.global_unstructured(\n",
    "        parameters,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=amount,\n",
    "    )\n",
    "\n",
    "    accuracy, t_2 = train_model(model, optimizer, criterion, loaders, N_epochs=30)\n",
    "\n",
    "    print('Amount : {} %, Valid accuracy : {:.5f}, time {:.1f} s'.format(\n",
    "        amount * 100, accuracy, t_1+t_2) )\n",
    "\n",
    "    acc.append(accuracy)\n",
    "    train_time.append(t_1 + t_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal brain damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet().to(device=device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(base_model.parameters(), betas = (0.851436, 0.999689), amsgrad=True, lr = 8e-5)\n",
    "loaders = {\"train\": trainloader, \"valid\": testloader}\n",
    "\n",
    "_, t_1 = train_model(base_model, optimizer, criterion, loaders, N_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "\n",
    "N = 10\n",
    "i = 0\n",
    "\n",
    "for x_batch, y_batch in trainloader:\n",
    "    i += 1\n",
    "\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "\n",
    "    outp = base_model(x_batch)\n",
    "    loss += criterion(outp, y_batch)\n",
    "    if i == N:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pth_jacobian(y, x, create_graph = False):\n",
    "    jac = []\n",
    "    flat_y = y.reshape(-1)\n",
    "    grad_y = torch.zeros_like(flat_y)\n",
    "    grad_y = torch.zeros_like(flat_y)\n",
    "    for i in range(len(flat_y)):\n",
    "        grad_y[i] = 1.\n",
    "        grad_x, = torch.autograd.grad(flat_y, x, grad_y, retain_graph=True, create_graph=create_graph)\n",
    "        grad_x = grad_x.reshape(x.shape)\n",
    "        jac.append(grad_x.reshape(x.shape))\n",
    "        grad_y[i] = 0.\n",
    "\n",
    "    return torch.stack(jac, axis = 0).reshape(y.shape + x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'conv1' : base_model.conv1[0],\n",
    "    'conv2' : base_model.conv2[0],\n",
    "    'res1_conv1' : base_model.res1[0][0],\n",
    "    'res1_conv2' : base_model.res1[1][0],\n",
    "    'conv3' : base_model.conv3[0],\n",
    "    'conv4' : base_model.conv4[0],\n",
    "    'res2_conv1' : base_model.res2[0][0],\n",
    "    'res2_conv2' : base_model.res2[1][0],\n",
    "    'fc' : base_model.classifier[2]\n",
    "}\n",
    "\n",
    "h_ii = {}\n",
    "s = {}\n",
    "# calculate diagonal elements of hessian\n",
    "for name, module in parameters.items():\n",
    "    weights = module.weight\n",
    "    grad = pth_jacobian(loss, weights, create_graph=True)\n",
    "\n",
    "    flat_y = grad.reshape(-1)\n",
    "    h_ii[name] = torch.zeros_like(flat_y)\n",
    "    for i in range(len(flat_y)):\n",
    "        grad_x, = torch.autograd.grad(flat_y[i], weights, retain_graph=True)\n",
    "        h_ii[name][i] = grad_x.reshape(-1)[i].item()\n",
    "\n",
    "    h_ii[name] = h_ii[name].reshape(weights.shape)\n",
    "    s_ii = 1/2 * h_ii[name]  * module.weight ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot amount over threshold plot\n",
    "import numpy as np\n",
    "\n",
    "s_t_list = np.logspace(-6, -2.6, 15)\n",
    "amount = []\n",
    "\n",
    "for s_th in s_t_list:\n",
    "\n",
    "    num = 0.\n",
    "\n",
    "    for name, s_ii in s_all.items():\n",
    "        num += torch.sum(s_ii > s_th).item()\n",
    "\n",
    "    num /= total_params\n",
    "    amount.append(num)\n",
    "\n",
    "plt.plot(s_t_list, np.array(amount) * total_params, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Importance threshold')\n",
    "plt.ylabel('Number of parameters')\n",
    "plt.savefig('./data/threshold.png', dpi = 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "import copy\n",
    "\n",
    "train_time = []\n",
    "acc = []\n",
    "\n",
    "for s_th in s_t_list:\n",
    "\n",
    "    model = copy.deepcopy(base_model)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 5e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    parameters = {\n",
    "      'conv1' : base_model.conv1[0],\n",
    "      'conv2' : base_model.conv2[0],\n",
    "      'res1_conv1' : base_model.res1[0][0],\n",
    "      'res1_conv2' : base_model.res1[1][0],\n",
    "      'conv3' : base_model.conv3[0],\n",
    "      'conv4' : base_model.conv4[0],\n",
    "      'res2_conv1' : base_model.res2[0][0],\n",
    "      'res2_conv2' : base_model.res2[1][0],\n",
    "      'fc' : base_model.classifier[2]\n",
    "    }\n",
    "\n",
    "    for name, module in parameters.items():\n",
    "        mask = s[name] >= s_th\n",
    "        torch.nn.utils.prune.CustomFromMask.apply(module, 'weight', mask)\n",
    "\n",
    "    accuracy, t_2 = train_model(model, optimizer, criterion, loaders, N_epochs=30)\n",
    "\n",
    "    print('s_th : {}, Valid accuracy : {:.5f}, time {:.1f} s'.format(\n",
    "        s_th, accuracy, t_1+t_2) )\n",
    "\n",
    "    acc.append(accuracy)\n",
    "    train_time.append(t_1 + t_2)\n",
    "\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 21:12:53) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f23c994d58cdfd8b0de51713bdc7b57a0b7adf9c2f2fa9c1f139bbd2bca952cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
